\documentclass[12pt,fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{enumitem}


\usepackage{comment}
%\usepackage[ruled,section]{algorithm}
%\usepackage[noend]{algorithmic}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом

\setlist[enumerate,itemize]{leftmargin=0pt,itemindent=2.7em}

\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[10mm]
        <<Задача кластеризации зашумлённых данных с неоднородной плотностью >>
    }\\[10mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Демин Георгий Александрович}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., профессор РАН\\
            \emph{Дьяконов Александр Геннадьевич}
        }
    \end{flushright}
    
    %\newenvironment{comment}{}{}
    
\begin{comment}
        \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
            Заведующий кафедрой\newline
            Математических Методов\newline
            Прогнозирования, академик РАН
            &
            ~\newline~\newline
            \hfill\hbox to 0.45\textwidth{\hrulefill~Ю. И. Журавлёв}
        \\[20mm]
            К защите допускаю\newline
            \hbox to 0.4\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2019 г.}
            &
            К защите рекомендую\newline
            \hbox to 0.45\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2019 г.}
        \end{tabular}
\end{comment}
    
    \vspace{\fill}
    Москва, 2019
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage
\begin{abstract}
    Выделение скоплений галактик - важная задача современной астрофизики. 
    Изучая распределение кластеров галактик (и сами кластеры) в космосе, можно исследовать вселенную и процессы, происходящие на разных этапах её развития. 
    
    В работе приведён обзор некоторых алгоритмов кластеризации, а также результаты их применения к решению задач кластеризации галактик.
    
    \textbf{Не готовы аннотация, часть экспериментов и вывод. }
    
\end{abstract}

\newpage
\section{Введение}
Задача кластеризации в машинном обучение - типичная задача обучения без учителя: есть некоторые критерии, которые сигнализирует о том хорошо решена задача или нет, но точной информации нет. Обычно эта задача является промежуточным этапом построения модели в машинном обучении. Она может использоваться для добавления к объектам признаков, уменьшения объёма данных путём объеденения объектов, попавших в один кластер. Также целью может служить - понимание структуры данных и их распределения. Более подробно задача кластеризации описана в разделе ''постановка задачи''.

В данной работе рассматривается задача кластеризация на данных, о которых известно, что большая их часть в кластеры не входит и является шумом. Кроме того, известно, что данные неоднородны и их плотность варируется. Предполагается также, что имеется некоторая априорная информация о распределении кластеров. Такая задача может возникнуть при распознавании образов, анализе биологических или физических систем.

Предлагается алгоритм, основанный на DBSCAN, учитывающий некоторые априорные представления о данных и адаптирующийся, способный настраиваться на варьирующуюся плотность. Алгоритм используется для решения задачи кластеризации галактик из каталога SDSS. Результаты его работы сравниваются с результатами, полученными с помощью ''классических'' алгоритмов (mean shift, классический DBSCAN)

Опишем более подробно задачу кластеризации и её типы.

\section{Постановка задачи}\label{problem}

\subsection{Задача кластеризации в машинном обучении}

Задача кластеризации в машинном обучении - это задача разбиения выборки объетов на некоторые множества, называющиеся кластерами, при этом одному множеству должны принадлежать в некотором роде похожие объекты, а объекты из разных кластеров - существенно отличаться. Входными данными для задачи кластеризации могут быть как объекты и их признаковое описание, так и матрица расстояний (или схожести) между объектами. 

Особенностью этой задачи является то, что её решение принципиально неоднозначно: нет точной постановки задачи, существует много критериев качества кластеризации, результат решения задачи сильно зависит от выбранной метрики (или схожести близости), качество кластеризации определяется во многом спецификой задачи (один и тот же результат может восприниматься как хороший или плохой в зависимости от того, для чего именно мы кластеризуем данные).


Для решения задачи могут применяться различные алгоритмы в зависимости от того, какой структура кластеров ожидается. На основании этого задачу кластеризации разделяют на (\cite{cluster_class}):
\begin{itemize}
\item \textbf{Иерархическая} или \textbf{плоская}. Под иерархической  структурой кластеров понимается такое их устройство, что некоторые мелкие кластеры могу быть объеденены в более крупные, которые в свою очередь также могут быть объеденены на более высоком уровне иерархии. Плоская кластеризация подразумевает, что некоторое множество выделенных кластеров не может само образовывать кластер.
\item \textbf{Исключающая}, \textbf{перекрывающая} или \textbf{нечёткая}. Исключающая кластеризация означает, что объект может быть отнесён только к одному классу, перекрывающая - к нескольким. Нечёткая кластеризация является частным случаем перекрывающей и для каждого объекта определяет вероятность вхождения в каждый кластер (другое её название - \textbf{вероятностная}).
\item \textbf{Полная} или \textbf{частичная}. При полной кластеризации каждый объект обязательно относится к кому-либо кластеру, а при частичной объект может являться шумом или выбросом и не должен принадлежать кластеру.
\item \textbf{Прототипная}, \textbf{графовая} или \textbf{плотностная}. При прототипной кластеризации кластер определяется как множество объектов, похожих на некий прототип. Графовая модель --- модель, при которой данные представлены в виде графа и кластером будет являться некое подмножество вершин, которые имеют большое количесвто связей или образуют отдельную связную компоненту. Плотностная кластеризация же означает, что кластером будут считаться объекты, плотность (при каком-то заданном распределении) которых больше средней плотности всей выборки.
\end{itemize}

В данной работе мы будем решать плоскую исключающую плотностную задачу кластеризации. Кроме того, ожидаем, что большая часть данных относится к шуму

\subsection{Формальная постановка задачи}
Приведём формальную постановку задачи: имеется $X \in \mathbb {R}^{N \times D}$ -  матрица объекты-признаки, $D \ll N$. $S = \{0, 1, ..., K \}$ - множество номеров кластеров, K (число кастеров) априорно неизвестно точно. Требуется найти отображение $y(x): X \rightarrow Y$, где $Y_i \in S$ и является номером кластера, которому принадлежит $X_i$.
 $y(x_0) = 0$ означает то, что объект $x_0$ является шумовым и не принадлжеит ни одному кластеру. Кроме того, мы предполагаем, что
\begin{itemize}
\item Кластеризация производится на основе плотности 

\item Число объектов, не относящихся ни к какому кластеру велико. 
\begin{equation}\label{more_noize}
         \frac{|\{x|y(x)=0\}|}{|X|} \geqslant 0.5
      \end{equation}. 
\item Имеется некое априорное представление о структуре кластеров
\begin{equation}\label{features}
         \exists  F_i(Y, K) \approx C_i, i = \overline{1, m}
      \end{equation}
\end{itemize}

\section{Обзор существующих решений}

\subsection{Сдвиг среднего значения (mean shift)}
Описание алгоритма mean shift

\subsection {DBSCAN}
Описание DBSCAN

Далее рассмотрим некоторые модификации алгоритма DBSCAN, на которые будет существенно опираться предложенный ниже подход

\subsubsection{VDBSCAN}
Для набора данных с варирующейся плотностью в статье \cite{VDBSCAN} был предложен следующий подход: запустить алгоритм DBSCAN несколько раз с разными значениями радиуса. При каждой последующей итерации исключать точки, кластеризованные на предыдущей, - таким образом могут быть обнаружены кластеры с различной плотностью. В статье \cite{AutoVDBSCAN} описывается способ для определения параметров k и eps. Предлагается посчитать среднее расстояние между точками в датасете, затем для каждой точки посчитать какой по номеру ближайший сосед отстоит от неё на это расстояние и взять в качестве
k - самый частый номер среди всех точек. Далее для каждой точки находится расстояние от неё до k-го ближайшего соседа, строится гистограмма таких расстояний и резкие скачки на ней будут считаться кандидатами на eps. 
У этого подхода есть несколько недостатков: во-первых, в нём слабо учитывается априорные знания о данных (только то, что они имеют варирующуюся плотность), во-вторых, он не подходит для датасета с большим количеством шума, так как гистограмма расстояний до k-го ближайшего соседа не позволит определить значения eps (она будет слишком гладкой из-за расстония до шумовых точек)

\subsubsection{Дифференциальная эволюция для подбора параметров DBSCAN}
Дифференциальная эволюция --- это метод стохастической оптимизации(см. \cite{dif_evol}). Его идея в том, что генерируется некоторое множество алгоритмов с разными параметрами - популяции (при этом используется один и тот же алгоритм и изменяются лишь его параметры), для каждого алгоритма вычисляется функция потерь. Алгоритмы, с ''удачной'' функцией потерь (то есть значение которой, оказалось низким), скрещиваются и получившийся алгоритм получает параметры, близкие к параметрам его предков. неудачные же алгоритмы с некоторой вероятностью мутируют, изменяя свои параметры. Несомненное преимущество этого метода в том, что он подходит для оптимизации вообще любых алгоритмов (однако вопрос о сходимости остаётся открытым). В \cite{BDE-DBSCAN} авторы используют этот подход, чтобы подобрать k и eps для алгоритма DBSCAN. Это довольно удачный способ для нашей постановки задачи, так как все \ref{features} могут быть учтены в функции потерь и параметры будут подобраны строго автоматически. Однако в этом методе тяжело учесть то, что данные неоднородные.

Для алгоритма DBSCAN есть множество усовершенствований, однако каждое из них имеет собственные недостатки, не позволяющие решить исследуемую задачу достаточно хорошо. Ниже будет представлен метод, опирающийся на приведённые выше модификации  DBSCAN.

\section{Предлагаемый подход}
В основе предлагаемого подхода лежит одна простая идея: для областей с разной плотностью нужно использовать алгоритм с разными параметрами. Сначала опишем метод, а затем приведём его формальное описание
\begin{enumerate} 
    \item Разбиваем признаковое пространство на подпространства с более-менее однородным распределением признаков(для этого можно построить распределение признаков и делить некоторые из них) 
    \item  Определяем некоторую функцию потерь, зависящую от (\ref{features}), с помощью неё на каждом подпространстве находим параметры алгоритма DBSCAN, при этом слишком маленькие по размеру выделенные кластеры относим к шуму, чтобы учесть (\ref{more_noize})
    \item Выделяем из подпространств зоны, на которых работа алгоритма нас не устраивает (это можно сделать на основе функции потерь или отдельных условий \ref{features}) И повторяем шаги независимо в каждой такой зоне.
\end{enumerate} 

\newpage

\begin{algorithm}
\caption{DBSAN for variety density}\label{alg:Example}
\begin{algorithmic}
\Function{TuneOnSubareas}{$subareas$}

\For{$X_i \in subareas$}
    \State eps, k  $\gets$ \Call{OptimizeDBSCAN}{$X_i$, LossFunc1}
    \State $labels \gets labels ~ \cup$ \Call{DBSCAN}{$X_i$, eps, k}
\EndFor

\State \Return $labels$
\EndFunction

\State $subareas \gets$ \Call{GetSubareas}{$X$}
\State $labels \gets \{\}$ 
\State $labels\_first\_step \gets \Call{TuneOnSubareas}{subareas}$
\State $bad\_labels \gets$ \Call{FindBadLabels}{labels, LossFunc2}
\State $bad\_subareas \gets subareas[ bad\_labels ]$
\State $labels\_first\_step \gets labels\_first\_step~\backslash~bad\_labels$
\State $labels\_result \gets labels ~ \cup$ \Call{TuneOnSubareas}{$bad\_subareas$}
\State \Return $labels\_result$

\end{algorithmic}
\end{algorithm}

Функция GetSubareas  неким образом разбивает выборку $X$ на подобасти
LossFunc1, LossFunc2 неким образом отражают апрорную информацию и штрафуют за нарушение (\ref{features}) 
OptimizeDBSCAN подбирает параметры для алгоритма DBSCAN на подвыборке, используя дифференциальную эволюцию, a DBSCAN применяет алгоритм с подобранными параметрами. Наконец, FindBadLabels определяет какие данные были кластеризованы плохо (в следствие различия в плотности или чего-то другого).

Далее покажем, как все можно составить функции потерь, используя априорные знания на реальных данных.

\section{Набор данных. Каталог SDSS}

Продемонстрируем предлагаемый алгоритм на задаче выделения галактик по каталогу SDSS(\cite{SDSS}). Этот каталог содержит сферические координаты и некоторые физические величины более чем 3 миллионах галактик, наблюдаемых в северном полушарии Земли. \textbf{Расстояние} до галактики определяется с помощью измерения красного смещения электромагнитных волн, излучаемых ею, близкими считаются галактики, имеющие красное смещение меньше 0.3 (всюду далее под расстоянием до галактики мы будем подразумевать красное смещение этой галактики). Особенностью этих данных является то, что для галактик сильно удалённых от Земли погрешность вычислений координат достигает больших значений - это не позволяет их анализировать. \textbf{Кластером} (скоплением или глобуларом) называется набор галактик, расположенных близко друг к другу (более точного определения дать не удаётся, так как объективные характеристики, по которым можно было бы определить что явлется кластером, а что нет, пока в астрофизике не определены). Считается также, что галактика принадлежит либо принадлежит только одному кластеру, либо не принадлежит никакому (таких галактик большинство и мы будем называть их шумовыми).
Ожидается, что кластеры будут сферической или элипсовидной формы

\section{Эксперименты}
\subsection{Априорные представления и их формализация}

В качестве априорных представлений о струтуре кластеров мы возьмём результаты работы Дж.~Эйбелла \cite{Abell}. Американский астроном вручную проанализировал фотопластины северной части звёздного неба и выделил 2712 кластеров. Так как это было сделано по фотометрическим данным, нельзя сказать, что координаты выделенных кластеров точны. Мы будем использовать 2 факта из работы физика: число кластеров приближённо известно и равно 1570\footnote{Каталог SDSS покрывает не все области видимого неба, поэтому ожидается меньшее число кластеров}, характерный размер кластера примерно 125-175 галактик.
Обозначим за $N(k)$ количество объектов в кластере с номером $k$, и фоормализуем априорную информацию таким образом
\begin{equation}\label{F_num_clusters}
  F_1(Y, K) = F_1(|Y|, K) = |K -  1572\cdot \frac{|Y|}{|X|}| - 50  \leq 0
\end{equation}
\begin{equation}\label{F_num_in1cluster}
  F_2(Y, K) =  F_2(N(1), \dots, N(K), K) = \frac{1}{K} \sum_{k=1}^K \left( | N(k) -150 | - 50 \right)  \leq 0
\end{equation}

Неравенства (\ref{F_num_clusters}) и (\ref{F_num_in1cluster}) не стоит понимать строго: это лишь наши предположения, они могут нарушаться. Исходя их этого построим функцию потерь для оптимизации DBSCAN. Будем штрафовать алгоритм, если он выделяет не то количество кластеров, которое нам нужно и если в кластерах оказывается намного больше или меньше галактик, чем предполагалось. Для этого хорошо подходит квадратичная функция. 
$$f_1(K) = a_1K^2 + b_1K + c_1$$
$$f_2(N(i)) = a_2N(i)^2 + b_2N(i) + c_2$$

Коэффициенты определим из условий
\begin{equation}\label{system_num_cl}
 \begin{cases}
   f_1|_{K=1572 \cdot \frac{|Y|}{|X|}} = 0
   \\
   f_1|_{K=1572 \cdot \frac{|Y|}{|X|} + 50} = 1
   \\
   f_1|_{K=1572 \cdot \frac{|Y|}{|X|} - 50} = 2.5
 \end{cases}
\end{equation}

\begin{equation}\label{system_num_in1cl}
 \begin{cases}
   f_2|_{N(i)=150} = 0
   \\
   f_2|_{N(i)=100} = 2
   \\
   f_2|_{N(i)=200} = 1
 \end{cases}
\end{equation}


Ясно, что получается 2 СЛАУ, коэффициенты которых легко находятся. Заметим, что мы полагаем квадратичную функцию равной нулю в точках с ожидаемым значением, и накладываем некоторый штраф в точках, удалённых на 50 от ожидаемого значения --- этот отступ, как и штрафы, вообще говоря, можно выбирать по-другому. Введём, наконец, функции потерь.

LossFunc2 $=\mathfrak{L}_2(Y, K) = \underset{i}{arg}\{N(i)>T\}$ 

LossFunc1 $= \mathfrak{L}_1(Y, K) = f_1(K) + \frac{1}{K} \sum_{i=1}^{K \backslash \mathfrak{L}_2(Y, K)}f_2(N(i))$

Эти функции с одной стороны учитывают априорные условия и с другой учитывают то, что данные имеют неоднородною плотность. Действительно, мы настраиваем DBSCAN таким образом, чтобы кластеры получались характерного небольшого размера, но если какой-то кластер получается слишком большим, мы его не учитываем и не штрафуем за него, вместо этого считаем, что распределение объектов, входящих в него имеет большую плотность и требуется для них подобрать другие параметры. Заметим, что мы никак не штрафуем количество галактик, которые определяются как шумовые, так как ограничения на количество объектов в каждом кластере и на количество самих кластеров по сути учитывают это.


возвращает номера кастеров, количество объектов в которых больше некоторого порога. Эта функция отражает условие неоднородности плотности данных: действительно, установив порог достаточно большим (например, в 10 раз больше ожидаемого числа кластеров), мы получим кластеры, в которых оказалось слишком много галактик. Ясно, что плотность распределения объектов, попавших в эти кластеры больше, чем средняя плотность по всей выборке, однако мы считаем, что это может



\subsection{Применённые алгоритмы}

Мы будем сравнивать результаты работы 3 основных алгоритмов
\begin{itemize}
    \item
        Mean shift
    \item
        Mini Batch K-means (спецификация K- means, которая для пересчёта центра кластера на каждой итерации использует не все объекты, принадлежащие ему, а лишь некоторую их подвыборку, что очень ускоряет алгоритм)
    \item
        Предложенный алгоритм оптимизации DBSCAN с дополнительным подбором параметров на областях с высокой плотностью
\end{itemize}

\subsection{Метрики качества}
Для задачи кластеризации с неизвестным числом кластеров существуют несколько коэффициентов, в какой-то мере описывающих резульат

\begin{itemize}
  \item Silhouette Coefficient
  \item Davies-Bouldin Index
  \item Calinski-Harabasz Index (Variance Ratio Criterion) 
  $$s(K)= \frac{Tr(B_k)}{Tr(W_k)} \times \frac{|X|-K}{K-1}$$
  где $B_k$ - матрица дисперсий центров кластеров, $W_k$ - сумма матриц дисперсий всех кластеров
  
  $$B_k = \sum_{k}^{}N(k)(c_k-c)(c_k-c)^\mathrm{T}$$
  $$W_k = \sum_{k=1}^{K}\sum_{x:y(x)=k}^{}(x-c_k)(x-c_k)^\mathrm{T}$$
  
  $c_k$ - центр $k$-го кластера, $c$ - центра всех объектов. Подразумевается, что метрика расстояний между объектами (и соответсвенно кластерами) - евклидова.
  Чем $s(K)$, тем лучше результат кластеризации. Дейсвительно максимизация $s(K)$ равносильна максимизации дисперсий распределения центров кластеров (то есть, чем более центры кластеров будут разбросаны, тем лучше) и в то же время минимизации дисперсий распределения в объектов в каждом кластере (чем компактнее расположены объекты, тем лучше). 
  
  Все эти 3 коэффициента дают лучшие значение для выпуклых сферических кластеров с евклидовой метрикой. Это подходит для нашей задачи.
   
   Отметим то, что обысно DBSCAN не склонен выделять кластеры сферической формы, его главное преимущество состоит в том, что он способен распознавать кластеры производной формы.
\end{itemize}

\subsection{Сравнения методов}
 
\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
                    & \# subareas & silhouette & (C-H) / $10^3$ & ($1 -$ D-B) \\ \hline
Mean Shift          & 4           & 0.34       & 21       & 0.15      \\ \hline
Mini Batch K-means  & 4           & 0.22       & \textbf{49}       & -0.1      \\ \hline
optimized DBSCAN    & 5           & 0.11       & 12       & 0.23      \\
optimized DBSCAN NH & 5           & \textbf{0.42}       & 41       & \textbf{0.32}      \\ \hline
optimized DBSCAN    & 4           & 0.13       & 17       & 0.23      \\
optimized DBSCAN NH & 4           & \textbf{0.49}       & \textbf{59}       & \textbf{0.4}       \\ \hline
\end{tabular}
\caption{Сравнение метод кластеризации}\label{all_methods}
\end{center}
\end{table}
 
 В Таблице \ref{all_methods} приведены результаты работы различных методов. Колонка \#subareas показывает насколько частей делилась исходная выборка; silhouette содержит значения соответствующего коэффициента; колонка C-H$/10^3$ заполнена значениями Calinski-Harabasz Index, делёнными на 1000; (1-DB) - Davies–Bouldin index, вычтенные из 1 (так как лучшей классификации соответствует меньшее значение, применяем такое преобразование для согласованности с другими индексами). optimized DBSCAN - это оптимизированный алгоритм без дополнительного подбора параметров на областях, соответствующих большим кластерам; optimized DBSCAN NH - с дополнительным подбором и разбиением соответсвующих областей.
 
Мы видим, что простой подбор 




\section{Заключение}

В~квалификационных работах последний раздел нужен для того, чтобы
конспективно перечислить основные результаты, полученные лично автором.

Результатами, в~частности, являются:
\begin{itemize}
\item
    Предложен новый подход к\dots
\item
    Разработан новый метод\dots, позволяющий\dots
\item
    Доказан ряд теорем, подтверждающих (опровергающих), что\dots
\item
    Проведены вычислительные эксперименты\dots,
    которые подтвердили / опровергли / привели к~новым постановкам задач.
\end{itemize}

Цель данного раздела: доказать квалификацию автора.
Даже беглого взгляда на заключение должно быть достаточно, чтобы стало ясно:
автору удалось решить актуальную, трудную, ранее не~решённую задачу,
предложенные автором решения обоснованы и~проверены.

Иногда в~Заключении приводится список направлений дальнейших исследований.

\newpage
Список литературы необходим в~любой научной публикации.
В дипломной работе он~обязателен.
Дурным тоном считается:
ссылаться на работы только одного-двух авторов (например, себя или шефа);
ссылаться на слишком малое число работ;
ссылаться только на очень старые работы;
ссылаться на работы, которых автор ни разу не видел;
ссылаться на~работы, которые не~упоминаются в~тексте
или которые не~имеют отношения к~данному тексту.
\section{Литература}

\begin{thebibliography}{2}

\bibitem{cluster_class}
M. Steinbach, V. Kumar, ''Cluster Analysis: Basic Concepts and Algorithms'', January 2005

\bibitem{VDBSCAN}
Peng Liu, Dong Zhou, Naijun Wu,  ''VDBSCAN: Varied Density Based Spatial Clustering of Applications with Noise'' // 2007 International Conference on Service Systems and Service Management

\bibitem{AutoVDBSCAN}
A. Özekes, F. Ozge Ozkok et al, ''AutoVDBSCAN: An Automatic and Level-Wise Varied-Density Based Anomaly Detection Algorithm'' //  2018
Conference: 7th International Conference on Advanced Technologies (ICAT'18)

\bibitem{BDE-DBSCAN}
A. Karami, R. Johansson, ''Choosing DBSCAN Parameters Automatically using
Differential Evolution'' // 2014 International Journal of Computer Applications (0975 8887) Volume 91 - No.

\bibitem{dif_evol}
R. M. Storn, K. Price, ''Differential Evolution: A Simple and Efficient Adaptive Scheme for Global Optimization Over Continuous Spaces'' // 1995 Journal of Global Optimization

\bibitem{SDSS}
SDSS Collaboration, ''THE THIRTEENTH DATA RELEASE OF THE SLOAN DIGITAL SKY SURVEY: FIRST SPECTROSCOPIC
DATA FROM THE SDSS-IV SURVEY MAPPING NEARBY GALAXIES AT APACHE POINT OBSERVATORY''

\bibitem{Abell}
Abell, G. O. (1958).''The distribution of rich clusters of galaxies. A catalogue of 2712 rich clusters found on the National Geographic Society Palomar Observatory Sky Survey''. The Astrophysical Journal Supplement Series, 3,: 211—288.


\bibitem{q}
q

\end{thebibliography}


\end{document}